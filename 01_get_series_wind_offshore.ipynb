{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import atlite\n",
    "from scipy import interpolate\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this code is to estimate generation from offshore wind farms using a virtual wind farm model. In this case, we have no best behaving power curve, as observations of offshore wind generation are non-existing at the moment. Therefore, we use a best guess turbine (provided by Charlene) and the estimated locations, as well as a hub height of 150 m (on the middle-to-low end of what was predicted a bit over 10 years ago and certainly a realistic value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load farm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic_offshore = pd.read_csv('../data/ic_offshore_wind_2030.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capacity_offshore = df_ic_offshore.rename({'lat':'y', 'lon':'x', 'ic':'Capacity'}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the ERA5 data and convert it into wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('1940-01-01', '2023-12-31', freq='1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base_wind = '../data/ERA5/ERA5_model_levels/u_v_{}_{:02d}.nc'\n",
    "path_base_geoh = '../data/ERA5/ERA5_model_levels/z_ml_{}_{:02d}.nc'\n",
    "\n",
    "path_wind_128 = '../data/ERA5/ERA5_model_levels/u_v_level128_{}_{:02d}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wind = xr.open_mfdataset([path_base_wind.format(date.year, date.month) for date in dates])\n",
    "ds_geoh = xr.open_mfdataset([path_base_geoh.format(date.year, date.month) for date in dates])\n",
    "\n",
    "ds_wind_128 = xr.open_mfdataset([path_wind_128.format(date.year, date.month) for date in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wind_128 = ds_wind_128.assign_coords({'level':128})\n",
    "ds_wind = xr.concat([ds_wind_128, ds_wind], dim='level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the surface roughness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_roughness_2018_2023 = xr.open_dataset('.../data/ERA5/ERA5_forecast_surface_roughness_geopotential_hourly_2018_2023.nc')\n",
    "ds_roughness_2018_2023 = ds_roughness_2018_2023.reduce(np.nanmean, dim='expver',keep_attrs=True)\n",
    "\n",
    "ds_roughness_2018_2023 = ds_roughness_2018_2023.drop_vars('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_roughness_1988_2017 = xr.open_mfdataset(['../data/ERA5/ERA5_forecast_surface_roughness_geopotential_hourly_{}_{}.nc'.format(yy, yy+5) for yy in np.arange(1988, 2013, 6, dtype=int)])\n",
    "\n",
    "ds_roughness_1988_2017 = ds_roughness_1988_2017.drop_vars('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_era5_old = []\n",
    "filepath_base = '../data/ERA5/ERA5_all_variables_{}_{:02d}.nc'\n",
    "for year in np.arange(1940,1988, dtype=int):\n",
    "    for month in np.arange(1,13, dtype=int):\n",
    "        list_files_era5_old.append(filepath_base.format(year, month))\n",
    "\n",
    "ds_1940_1987 = xr.open_mfdataset(list_files_era5_old)\n",
    "\n",
    "ds_1940_1987 = ds_1940_1987[['fsr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_roughness = xr.concat([ds_1940_1987, ds_roughness_1988_2017, ds_roughness_2018_2023], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_roughness = ds_roughness['fsr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_wind['longitude']  = ds_wind['longitude'] - 360.\n",
    "ds_geoh['longitude']  = ds_geoh['longitude'] - 360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (ds_geoh['z']/9.80665).rename({'hybrid':'level'})\n",
    "u = ds_wind['u']\n",
    "v = ds_wind['v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z - z[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the order of dimensions and the number of levels\n",
    "u = u.transpose(*(z.dims))\n",
    "v = v.transpose(*(z.dims))\n",
    "z = z[:,:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only the points nearest to the farms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nearest_point_mask(dataarray, target_lats, target_lons):\n",
    "    \"\"\"\n",
    "    Create a boolean mask where the nearest grid points to each lat and lon in the lists are True, and False otherwise.\n",
    "\n",
    "    Parameters:\n",
    "    - dataarray (xarray.DataArray): Input data array with lat/lon dimensions.\n",
    "    - target_lats (list or array): List of target latitudes.\n",
    "    - target_lons (list or array): List of target longitudes.\n",
    "\n",
    "    Returns:\n",
    "    - mask (xarray.DataArray): A boolean mask with True at the nearest grid points, False elsewhere.\n",
    "    \"\"\"\n",
    "    # Ensure target latitudes and longitudes are the same length\n",
    "    if len(target_lats) != len(target_lons):\n",
    "        raise ValueError(\"The number of target latitudes and longitudes must match.\")\n",
    "\n",
    "    # Extract lat and lon arrays from the DataArray\n",
    "    lats = dataarray['latitude'].values\n",
    "    lons = dataarray['longitude'].values\n",
    "\n",
    "    # Create an empty mask with the same shape as the lat/lon grid\n",
    "    mask = np.full((len(lats), len(lons)), False)\n",
    "\n",
    "    # Create 2D grids of lat/lon\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "\n",
    "    # Loop over each pair of target lat and lon\n",
    "    for target_lat, target_lon in zip(target_lats, target_lons):\n",
    "        # Calculate the Euclidean distance to all grid points\n",
    "        distances = np.sqrt((lat_grid - target_lat)**2 + (lon_grid - target_lon)**2)\n",
    "\n",
    "        # Find the index of the minimum distance (nearest point)\n",
    "        nearest_idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "\n",
    "        # Set the nearest point to True in the mask\n",
    "        mask[nearest_idx] = True\n",
    "\n",
    "    # Return the mask as a DataArray with the same lat/lon coordinates as the input dataarray\n",
    "    return xr.DataArray(mask, coords={'latitude': lats, 'longitude': lons}, dims=['latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sel = u.where(create_nearest_point_mask(u, df_capacity_offshore['y'], df_capacity_offshore['x']))\n",
    "v_sel = v.where(create_nearest_point_mask(v, df_capacity_offshore['y'], df_capacity_offshore['x']))\n",
    "z_sel = z.where(create_nearest_point_mask(z, df_capacity_offshore['y'], df_capacity_offshore['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the levels to use for the calculation at 150 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aina/anaconda3/envs/atlite/lib/python3.11/site-packages/dask/array/reductions.py:618: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.nanmin(x_chunk, axis=axis, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Define the target height\n",
    "target_height = 150.0\n",
    "\n",
    "# Step 1: Calculate min and max heights for each point to identify boundary conditions\n",
    "min_z = z_sel.min(dim='level').compute()\n",
    "max_z = z_sel.max(dim='level').compute()\n",
    "\n",
    "# Step 2: Create masks to handle cases where the target height is below min or above max\n",
    "below_min_mask = target_height <= min_z\n",
    "above_max_mask = target_height >= max_z\n",
    "\n",
    "# Step 3: Replace NaNs in over and under levels to avoid errors during argmin/argmax\n",
    "# For the level_over, replace NaNs with max level index if height is above max, else fill as needed\n",
    "z_above = (z_sel - target_height).where((z_sel - target_height) >= 0, np.inf)\n",
    "level_over = z_above.argmin(dim='level')\n",
    "\n",
    "# For the level_under, replace NaNs with min level index if height is below min, else fill as needed\n",
    "z_below = (z_sel - target_height).where((z_sel - target_height) <= 0, -np.inf)\n",
    "level_under = z_below.argmax(dim='level')\n",
    "\n",
    "# Step 4: Compute the levels and handle boundary cases\n",
    "level_over = level_over.compute().astype(int)\n",
    "level_under = level_under.compute().astype(int)\n",
    "\n",
    "# Use the lowest level when target height is below min; highest level when above max\n",
    "level_over = xr.where(below_min_mask, 0, level_over)  # If below min, take lowest level\n",
    "level_under = xr.where(above_max_mask, z.level.size - 1, level_under)  # If above max, take highest level\n",
    "\n",
    "# Ensure indices are within the bounds of the array\n",
    "level_over = np.clip(level_over, 0, z.level.size - 1)\n",
    "level_under = np.clip(level_under, 0, z.level.size - 1)\n",
    "\n",
    "# Step 5: Convert indices to the correct shape for advanced indexing\n",
    "# Expand the indices to match the shape of (time, lat, lon, 1) for use with np.take_along_axis\n",
    "expanded_level_over = level_over.expand_dims(dim='level', axis=1)\n",
    "expanded_level_under = level_under.expand_dims(dim='level', axis=1)\n",
    "\n",
    "# Step 6: Convert to NumPy arrays for indexing\n",
    "z_data = z_sel.compute().data\n",
    "u_data = u_sel.compute().data\n",
    "v_data = v_sel.compute().data\n",
    "\n",
    "# Step 7: Extract above and below values using advanced indexing\n",
    "z_over = np.take_along_axis(z_data, expanded_level_over, axis=1).squeeze(axis=1)\n",
    "z_under = np.take_along_axis(z_data, expanded_level_under, axis=1).squeeze(axis=1)\n",
    "u_over = np.take_along_axis(u_data, expanded_level_over, axis=1).squeeze(axis=1)\n",
    "u_under = np.take_along_axis(u_data, expanded_level_under, axis=1).squeeze(axis=1)\n",
    "v_over = np.take_along_axis(v_data, expanded_level_over, axis=1).squeeze(axis=1)\n",
    "v_under = np.take_along_axis(v_data, expanded_level_under, axis=1).squeeze(axis=1)\n",
    "\n",
    "# Step 8: Calculate distances from the target height\n",
    "distance_over = z_over - target_height\n",
    "distance_under = target_height - z_under\n",
    "\n",
    "# Replace zero distances with NaNs to avoid division by zero\n",
    "distance_over = np.where(distance_over != 0, distance_over, np.nan)\n",
    "distance_under = np.where(distance_under != 0, distance_under, np.nan)\n",
    "\n",
    "# Step 9: Calculate inverse distance weights\n",
    "weight_over = 1.0 / distance_over\n",
    "weight_under = 1.0 / distance_under\n",
    "\n",
    "# If no valid level above or below, set the weights of the closest level to 1\n",
    "weight_over = np.nan_to_num(weight_over, nan=1.0)\n",
    "weight_under = np.nan_to_num(weight_under, nan=1.0)\n",
    "\n",
    "# Step 10: Calculate 100-meter wind components using inverse distance weighting\n",
    "u_150 = (weight_over * u_over + weight_under * u_under) / (weight_over + weight_under)\n",
    "v_150 = (weight_over * v_over + weight_under * v_under) / (weight_over + weight_under)\n",
    "\n",
    "# Convert the results back into xarray DataArrays with the appropriate dimensions\n",
    "u_150 = xr.DataArray(u_150, coords=[z.coords['time'], z.coords['latitude'], z.coords['longitude']], dims=['time', 'latitude', 'longitude'])\n",
    "v_150 = xr.DataArray(v_150, coords=[z.coords['time'], z.coords['latitude'], z.coords['longitude']], dims=['time', 'latitude', 'longitude'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with the variables necessary for the cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_150.attrs['units'] = 'm s**-1'\n",
    "v_150.attrs['units'] = 'm s**-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({'u100':u_150, 'v100':v_150, 'fsr':da_roughness}) # we assign the wind to 100 m in the variable name to avoid an error, but it is actually interpolated to 200 m. Along these lines, we will have to tell atlite that the turbine is at 100 m so that it does not vertically extrapolate the wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_ie = atlite.cutout.get_cutout_from_era5_data('path', ds, ['wind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Cutout \"path\">\n",
       " x = -11.00 ⟷ -5.00, dx = 0.25\n",
       " y = 51.00 ⟷ 56.00, dy = 0.25\n",
       " time = 1940-01-01 ⟷ 2023-12-31, dt = H\n",
       " module = era5\n",
       " prepared_features = ['wind']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutout_ie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the power curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_power_curve_raw = '../../ramping/data/data_others/IEA_15MW_240_RWT.csv'\n",
    "df_power_curve_raw = pd.read_csv(path_power_curve_raw)[['Wind Speed [m/s]', 'Power [kW]']]\n",
    "df_power_curve_raw = df_power_curve_raw.rename({'Wind Speed [m/s]':'windspeed', 'Power [kW]':'power'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input: wind speeds and corresponding power values\n",
    "wind_speeds_input = df_power_curve_raw['windspeed'].values  # Example wind speeds\n",
    "power_values_input = df_power_curve_raw['power'].values  # Corresponding power values\n",
    "\n",
    "# Define the full wind speed range from 0 to 40 m/s with 0.01 m/s intervals\n",
    "wind_speeds_interp = np.arange(0, 40.01, 0.01)\n",
    "\n",
    "# Interpolation\n",
    "# Create an interpolation function based on the input data\n",
    "interp_function = interpolate.interp1d(wind_speeds_input, power_values_input, kind='cubic', fill_value=0, bounds_error=False)\n",
    "\n",
    "# Apply the interpolation function to the new wind speed range\n",
    "power_values_interp = interp_function(wind_speeds_interp)\n",
    "\n",
    "# Ensure power is 0 before the first and after the last known wind speed\n",
    "first_wind_speed = wind_speeds_input[0]\n",
    "last_wind_speed = wind_speeds_input[-1]\n",
    "\n",
    "power_values_interp[wind_speeds_interp < first_wind_speed] = 0\n",
    "power_values_interp[wind_speeds_interp > last_wind_speed] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth the power curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian filter function Γ(n, σ)\n",
    "def gaussian_filter(n, sigma):\n",
    "    return (1 / (math.sqrt(2 * np.pi) * sigma)) * np.exp(-n**2 / (2 * sigma**2))\n",
    "\n",
    "# Function to smooth the power curve with a selected smoothing level\n",
    "def smooth_power_curve(unsmoothed_curve, wind_speeds, smoothing_level):\n",
    "    smoothed_curve = np.zeros_like(unsmoothed_curve)  # Initialize smoothed power curve array\n",
    "    step = 0.01  # Resolution of 0.01 m/s\n",
    "\n",
    "    for i, x in enumerate(wind_speeds):\n",
    "        # Adjust σ based on wind speed and the selected smoothing level\n",
    "        sigma = 0.6 + 0.2 * smoothing_level * x\n",
    "\n",
    "        # Summing from -4σ to +4σ\n",
    "        aggregate_power = 0\n",
    "        normalization_factor = 0  # To normalize the Gaussian weighting\n",
    "        \n",
    "        for n in np.arange(-4 * sigma, 4 * sigma, step):\n",
    "            shifted_index = i - int(n / step)  # Find the shifted index corresponding to (x - n)\n",
    "            if 0 <= shifted_index < len(unsmoothed_curve):\n",
    "                PCT_value = unsmoothed_curve[shifted_index]\n",
    "            else:\n",
    "                PCT_value = 0  # Out of bounds: assume power curve is 0\n",
    "            \n",
    "            weight = gaussian_filter(n, sigma)\n",
    "            aggregate_power += PCT_value * weight\n",
    "            normalization_factor += weight  # Sum of weights for normalization\n",
    "\n",
    "        smoothed_curve[i] = aggregate_power / normalization_factor  # Normalize the sum by weights\n",
    "\n",
    "    return smoothed_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply smoothing with different smoothing levels\n",
    "smoothing_level = 0.20\n",
    "smoothed_power_curve = smooth_power_curve(power_values_interp, wind_speeds_interp, smoothing_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate wind generation and CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine = {'hub_height':100, 'P':15000., 'V':wind_speeds_interp, 'POW':smoothed_power_curve} # we assign the turbine at 100m because we have to trick atlite into thinking the wind is at 100 m to avoid an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = cutout_ie.layout_from_capacity_list(df_capacity_offshore)\n",
    "wind = cutout_ie.wind(turbine=turbine, layout=layout, add_cutout_windspeed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_cf = wind[0]/layout.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_cf.to_netcdf('../data/wind_offshore_cf_1940_2023.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
